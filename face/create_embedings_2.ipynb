{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(cv2.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained face recognition model (e.g., FaceNet)\n",
    "# model = load_model('models/mobile_face_net.tflite')\n",
    "\n",
    "# Initialize the TensorFlow Lite Interpreter for MobileFaceNet\n",
    "interpreter = tf.lite.Interpreter(model_path='models/mobile_face_net.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load a pre-trained SSD model for face detection\n",
    "ssd_model = tf.lite.Interpreter(model_path='models/lite-model_ssd_mobilenet_v2_100_fp32_default_1.tflite')\n",
    "ssd_model.allocate_tensors()\n",
    "ssd_input_details = ssd_model.get_input_details()\n",
    "ssd_output_details = ssd_model.get_output_details()\n",
    "\n",
    "# Folder containing multiple images of the same face\n",
    "image_folder = 'input_images'\n",
    "\n",
    "# An output file to store the extracted embeddings\n",
    "output_file = 'embeddings/embeddings.json'\n",
    "\n",
    "# A user name of the input images\n",
    "user_name = 'Mussa'\n",
    "\n",
    "# Create a list to store embeddings\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract bounding boxes from SSD detection results\n",
    "def extract_face_boxes(detection_results, image_shape, confidence_threshold=0.5):\n",
    "    height, width = image_shape\n",
    "    bounding_boxes = []\n",
    "    for detection in detection_results[0]:\n",
    "        confidence = detection[2]\n",
    "        if confidence > confidence_threshold:\n",
    "            left = int(detection[3] * width)\n",
    "            top = int(detection[4] * height)\n",
    "            right = int(detection[5] * width)\n",
    "            bottom = int(detection[6] * height)\n",
    "            bounding_boxes.append((left, top, right - left, bottom - top))\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Perform face detection with the \"ssd_mobilenet_v2.tflite\" model\n",
    "    input_image = cv2.resize(image, (224, 224))  # Resize the image to match the model's input size\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    input_image = input_image.astype(np.float32)  # Convert to FLOAT32\n",
    "    input_image /= 255.0  # Normalize pixel values to [0, 1]\n",
    "    input_image = np.expand_dims(input_image, axis=0)  # Add batch dimension\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(cv2.cvtColor(input_image[0], cv2.COLOR_BGR2RGB), cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    plt.title('Input image')\n",
    "    plt.show()\n",
    "\n",
    "    ssd_model.set_tensor(ssd_input_details[0]['index'], input_image)\n",
    "    ssd_model.invoke()\n",
    "    detection_results = ssd_model.get_tensor(ssd_output_details[0]['index'])\n",
    "    \n",
    "    # Extract bounding boxes of detected faces\n",
    "    bounding_boxes = extract_face_boxes(detection_results, image.shape[:2])\n",
    "    \n",
    "    if not bounding_boxes:\n",
    "        print(f\"No faces detected in {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract and preprocess each detected face\n",
    "    face_images = []\n",
    "    for (x, y, w, h) in bounding_boxes:\n",
    "        face_image = image[y:y+h, x:x+w]  # Crop the detected face region\n",
    "        \n",
    "        # Preprocess the face image for the MobileFaceNet model\n",
    "        face_image = cv2.resize(face_image, (112, 112))\n",
    "        face_image = (face_image - 127.5) / 128.0  # Normalize pixel values to [-1, 1]\n",
    "        face_image = np.float32(face_image)  # Convert to FLOAT32\n",
    "        face_image = np.expand_dims(face_image, axis=0)  # Add batch dimension\n",
    "        \n",
    "        face_images.append(face_image)\n",
    "    \n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for face recognition\n",
    "def recognize_faces(face_images):\n",
    "    # Process each detected face\n",
    "    for i, face_image in enumerate(face_images):\n",
    "        # Perform inference with the MobileFaceNet model\n",
    "        interpreter.set_tensor(input_details[0]['index'], face_image)\n",
    "        interpreter.invoke()\n",
    "        embeddings = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Normalize the embeddings to have unit length\n",
    "        embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Display the preprocessed face image using Matplotlib\n",
    "        # plt.imshow(face_image[0, :, :, 0], cmap='gray')  # Display in grayscale\n",
    "        plt.imshow(cv2.cvtColor(cv2.cvtColor(face_image[0], cv2.COLOR_BGR2RGB), cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Face {i+1}')\n",
    "        plt.show()\n",
    "        \n",
    "        # You can further process or store the embeddings here\n",
    "        # Example: Store embeddings in a list for later use\n",
    "        all_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all image files in the folder\n",
    "image_files = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder) if filename.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the image files, detect faces, and recognize them\n",
    "for image_path in image_files:\n",
    "    face_images = preprocess_image(image_path)\n",
    "    if face_images is not None:\n",
    "        recognize_faces(face_images)\n",
    "\n",
    "print(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average embedding (if needed)\n",
    "if all_embeddings:\n",
    "\n",
    "    # Calculate the average embedding\n",
    "    average_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    # Store the average embedding in a JSON file\n",
    "    data = {\n",
    "        'name': user_name,\n",
    "        'embedding': average_embedding.tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "        print(data)\n",
    "\n",
    "    print(f'Done creating embeddings for user: {user_name}')\n",
    "\n",
    "else: \n",
    "    print(\"No embeddings generated for the input images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
