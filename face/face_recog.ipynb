{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "# from scipy.spatial import distance\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "import Facenet\n",
    "# from deepface import DeepFace\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An input file to get stored embeddings\n",
    "input_file = 'embeddings/embeddings.json'\n",
    "\n",
    "model_file = 'models/facenet_weights.h5'\n",
    "\n",
    "cv_model_file = 'models/haarcascade_frontalface_default.xml'\n",
    "\n",
    "WIDTH = 640\n",
    "HEIGHT = 480\n",
    "\n",
    "# Set the cosine similarity threshold for recognition\n",
    "threshold = 0.7  # Adjust this value as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_embeddings = []\n",
    "\n",
    "# Load the embeddings from the JSON file\n",
    "with open(input_file, 'r') as json_file:\n",
    "    stored_data = json.load(json_file)\n",
    "\n",
    "    # Create a dictionary to store the embeddings by name\n",
    "    stored_embeddings = {}\n",
    "    for item in stored_data:\n",
    "        name = item['name']\n",
    "        embeddings = item['embeddings']\n",
    "        stored_embeddings[name] = []\n",
    "        for timestamp, embedding in embeddings.items():\n",
    "            stored_embeddings[name].append(embedding)\n",
    "\n",
    "print(stored_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract faces\n",
    "def extract_faces(img_path):\n",
    "    print(f\"Extracting faces for image: {img_path}\")\n",
    "    imgs = DeepFace.extract_faces(img_path=img_path, enforce_detection=False)\n",
    "    return imgs\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(img_path):\n",
    "    # print(f\"Creating embeddings for image: {img_path}\")\n",
    "    print(f\"Creating embeddings...\")\n",
    "    embedding_objs = DeepFace.represent(img_path=img_path, model_name=\"Facenet\", enforce_detection=False)\n",
    "    return embedding_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the FaceNet model\n",
    "    face_net_model = Facenet.loadModel()\n",
    "    # Load a pre-trained face detection model (e.g., Haar Cascade or dlib)\n",
    "    face_cascade = cv2.CascadeClassifier(cv_model_file)\n",
    "\n",
    "    # Create a VideoCapture object to capture video from the default camera (usually 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3,WIDTH)\n",
    "    cap.set(4,HEIGHT)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the camera\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame in a window named 'Video Capture'\n",
    "        # cv2.imshow('Video Capture', frame)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face ROI\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Preprocess the face image for FaceNet\n",
    "            face_img = cv2.resize(face_roi, (160, 160))\n",
    "            face_img = np.expand_dims(face_img, axis=0)\n",
    "            face_img = (face_img - 127.5) / 128.0  # Normalize\n",
    "\n",
    "            # Extract embeddings from the face using FaceNet\n",
    "            face_embeddings = face_net_model.predict(face_img)\n",
    "            # face_embeddings = []\n",
    "            # embeddings = create_embeddings(frame)\n",
    "            # if len(embeddings) > 0:\n",
    "            #     for embedding in embeddings:\n",
    "            #         face_embeddings.append(embedding[\"embedding\"])\n",
    "            \n",
    "            # Compare the embeddings with stored embeddings for each user\n",
    "            min_distance = float(\"inf\")\n",
    "            recognized_face = None\n",
    "\n",
    "            for name, stored_user_embeddings in stored_embeddings.items():\n",
    "                for stored_embedding in stored_user_embeddings:\n",
    "                    # # Calculate the distance between the embeddings\n",
    "                    # dist = distance.euclidean(stored_embedding, face_embeddings[0])\n",
    "                    # Calculate the Euclidean distance between the embeddings\n",
    "                    dist = np.linalg.norm(stored_embedding - face_embeddings[0])\n",
    "                    \n",
    "                    if dist < min_distance:\n",
    "                        min_distance = dist\n",
    "                        recognized_face = name\n",
    "            \n",
    "            print(\"distance: \", min_distance)\n",
    "            if min_distance > threshold:  # You can set a threshold for recognition\n",
    "                cv2.putText(frame, recognized_face, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, 'Unknown', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "            # # Compare the embeddings with stored embeddings for each user using cosine similarity\n",
    "            # max_similarity = -1.0\n",
    "            # recognized_face = None\n",
    "\n",
    "            # for name, stored_user_embeddings in stored_embeddings.items():\n",
    "            #     for stored_embedding in stored_user_embeddings:\n",
    "            #         # Calculate cosine similarity between the embeddings\n",
    "            #         similarity = cosine_similarity([stored_embedding], [face_embeddings[0]])[0][0]\n",
    "                    \n",
    "            #         if similarity > max_similarity:\n",
    "            #             max_similarity = similarity\n",
    "            #             recognized_face = name\n",
    "            \n",
    "            # if max_similarity > threshold:  # Check against the threshold for recognition\n",
    "            #     cv2.putText(frame, recognized_face, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            # else:\n",
    "            #     cv2.putText(frame, 'Unknown', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "        # Listen for a key press and check if it's the \"Esc\" key (27 is the ASCII code for Esc)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "    # Release the VideoCapture and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
