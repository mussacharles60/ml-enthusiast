{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained face recognition model (e.g., FaceNet)\n",
    "# model = load_model('models/mobile_face_net.tflite')\n",
    "\n",
    "# Initialize the TensorFlow Lite Interpreter for MobileFaceNet\n",
    "interpreter = tf.lite.Interpreter(model_path='models/mobile_face_net.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load a pre-trained face detection model (e.g., Haar Cascade or MTCNN)\n",
    "face_cascade = cv2.CascadeClassifier('models/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Folder containing multiple images of the same face\n",
    "image_folder = 'input_images'\n",
    "\n",
    "# An output file to store the extracted embeddings\n",
    "output_file = 'embeddings/embeddings.json'\n",
    "\n",
    "# A user name of the input images\n",
    "user_name = 'Mussa'\n",
    "\n",
    "# Create a list to store embeddings\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Converting the image to grayscale\n",
    "    gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(cv2.cvtColor(gray_img, cv2.COLOR_BGR2RGB), cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis('off')\n",
    "    plt.title('Input image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=9, minSize=(40, 40))\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(f\"No faces detected in {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract and preprocess each detected face\n",
    "    face_images = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_image = image[y:y+h, x:x+w]  # Crop the detected face region\n",
    "        \n",
    "        # Preprocess the face image for the MobileFaceNet model\n",
    "        face_image = cv2.resize(face_image, (112, 112))\n",
    "        face_image = (face_image - 127.5) / 128.0  # Normalize pixel values to [-1, 1]\n",
    "        face_image = np.float32(face_image)  # Convert to FLOAT32\n",
    "        face_image = np.expand_dims(face_image, axis=0)  # Add batch dimension\n",
    "        \n",
    "        face_images.append(face_image)\n",
    "\n",
    "    return face_images\n",
    "\n",
    "# Define a function for face recognition\n",
    "def recognize_faces(face_images):\n",
    "    # Process each detected face\n",
    "    for i, face_image in enumerate(face_images):\n",
    "        # Perform inference with the MobileFaceNet model\n",
    "        interpreter.set_tensor(input_details[0]['index'], face_image)\n",
    "        interpreter.invoke()\n",
    "        embeddings = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Normalize the embeddings to have unit length\n",
    "        embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Display the preprocessed face image using Matplotlib\n",
    "        plt.imshow(face_image[0, :, :, 0], cmap='gray')  # Display in grayscale\n",
    "        # plt.axis('off')\n",
    "        plt.title(f'Face {i+1}')\n",
    "        plt.show()\n",
    "        \n",
    "        # You can further process or store the embeddings here\n",
    "        # Example: Store embeddings in a list for later use\n",
    "        all_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all image files in the folder\n",
    "image_files = [os.path.join(image_folder, filename) for filename in os.listdir(image_folder) if filename.endswith('.jpg')]\n",
    "\n",
    "print(image_files)\n",
    "\n",
    "# Iterate through the image files, detect faces, and recognize them\n",
    "for image_path in image_files:\n",
    "    face_images = preprocess_image(image_path)\n",
    "    if face_images is not None:\n",
    "        recognize_faces(face_images)\n",
    "\n",
    "print(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average embedding (if needed)\n",
    "if all_embeddings:\n",
    "\n",
    "    # Calculate the average embedding\n",
    "    average_embedding = np.mean(all_embeddings, axis=0)\n",
    "    \n",
    "    # Store the average embedding in a JSON file\n",
    "    data = {\n",
    "        'name': user_name,\n",
    "        'embedding': average_embedding.tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "        print(data)\n",
    "\n",
    "    print(f'Done creating embeddings for user: {user_name}')\n",
    "\n",
    "else: \n",
    "    print(\"No embeddings generated for the input images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
