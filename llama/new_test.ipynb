{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******\n",
      "Could not load OpenAI model. Using default LlamaCPP=llama2-13b-chat. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
      "Original error:\n",
      "No API key found for OpenAI.\n",
      "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
      "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\n",
      "******\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import llama_cpp library.Please install llama_cpp with `pip install llama-cpp-python`.See the full installation guide for GPU support at `https://github.com/abetlen/llama-cpp-python`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\llms\\llama_cpp.py:86\u001b[0m, in \u001b[0;36mLlamaCPP.__init__\u001b[1;34m(self, model_url, model_path, temperature, max_new_tokens, context_window, messages_to_prompt, completion_to_prompt, callback_manager, generate_kwargs, model_kwargs, verbose)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[0;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\llama_index\\examples\\paul_graham_essay\\new_test.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/llama_index/examples/paul_graham_essay/new_test.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/llama_index/examples/paul_graham_essay/new_test.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m documents \u001b[39m=\u001b[39m SimpleDirectoryReader(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mload_data()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/llama_index/examples/paul_graham_essay/new_test.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(documents)\n",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\indices\\base.py:92\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create index from documents.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m storage_context \u001b[39m=\u001b[39m storage_context \u001b[39mor\u001b[39;00m StorageContext\u001b[39m.\u001b[39mfrom_defaults()\n\u001b[1;32m---> 92\u001b[0m service_context \u001b[39m=\u001b[39m service_context \u001b[39mor\u001b[39;00m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults()\n\u001b[0;32m     93\u001b[0m docstore \u001b[39m=\u001b[39m storage_context\u001b[39m.\u001b[39mdocstore\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m service_context\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mas_trace(\u001b[39m\"\u001b[39m\u001b[39mindex_construction\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\indices\\service_context.py:155\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot specify both llm and llm_predictor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m     llm \u001b[39m=\u001b[39m resolve_llm(llm)\n\u001b[1;32m--> 155\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor(llm\u001b[39m=\u001b[39;49mllm)\n\u001b[0;32m    156\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[0;32m    157\u001b[0m     llm_predictor\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\llm_predictor\\base.py:92\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[1;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     86\u001b[0m     llm: Optional[LLMType] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m     query_wrapper_prompt: Optional[BasePromptTemplate] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m resolve_llm(llm)\n\u001b[0;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m callback_manager:\n\u001b[0;32m     95\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\llms\\utils.py:40\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m is_local \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     38\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mllm must start with str \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or of type LLM or BaseLanguageModel\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         )\n\u001b[1;32m---> 40\u001b[0m     llm \u001b[39m=\u001b[39m LlamaCPP(\n\u001b[0;32m     41\u001b[0m         model_path\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[0;32m     42\u001b[0m         messages_to_prompt\u001b[39m=\u001b[39;49mmessages_to_prompt,\n\u001b[0;32m     43\u001b[0m         completion_to_prompt\u001b[39m=\u001b[39;49mcompletion_to_prompt,\n\u001b[0;32m     44\u001b[0m         model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mn_gpu_layers\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m},\n\u001b[0;32m     45\u001b[0m     )\n\u001b[0;32m     46\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm, BaseLanguageModel):\n\u001b[0;32m     47\u001b[0m     \u001b[39m# NOTE: if it's a langchain model, wrap it in a LangChainLLM\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     llm \u001b[39m=\u001b[39m LangChainLLM(llm\u001b[39m=\u001b[39mllm)\n",
      "File \u001b[1;32mD:\\Projects\\llama_index\\llama_index\\llms\\llama_cpp.py:88\u001b[0m, in \u001b[0;36mLlamaCPP.__init__\u001b[1;34m(self, model_url, model_path, temperature, max_new_tokens, context_window, messages_to_prompt, completion_to_prompt, callback_manager, generate_kwargs, model_kwargs, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mllama_cpp\u001b[39;00m \u001b[39mimport\u001b[39;00m Llama\n\u001b[0;32m     87\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import llama_cpp library.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install llama_cpp with `pip install llama-cpp-python`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the full installation guide for GPU support at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`https://github.com/abetlen/llama-cpp-python`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     95\u001b[0m model_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m     96\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mn_ctx\u001b[39m\u001b[39m\"\u001b[39m: context_window, \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m: verbose},\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(model_kwargs \u001b[39mor\u001b[39;00m {}),  \u001b[39m# Override defaults via model_kwargs\u001b[39;00m\n\u001b[0;32m     98\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[39m# check if model is cached\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import llama_cpp library.Please install llama_cpp with `pip install llama-cpp-python`.See the full installation guide for GPU support at `https://github.com/abetlen/llama-cpp-python`"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
