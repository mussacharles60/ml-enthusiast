{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import wave\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available input audio devices and their names\n",
    "p = pyaudio.PyAudio()\n",
    "print(\"Available input devices:\")\n",
    "for i in range(p.get_device_count()):\n",
    "    device_info = p.get_device_info_by_index(i)\n",
    "    print(f\"{i}: {device_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the desired input device by its ID (replace with the appropriate ID)\n",
    "selected_device_id = 1  # Change this to the ID of your desired input device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for audio recording\n",
    "sample_rate = 16000  # You can adjust this based on your needs\n",
    "duration = 5  # Recording duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio from the selected input device\n",
    "print(f\"Recording audio from {p.get_device_info_by_index(selected_device_id)['name']}. Please speak...\")\n",
    "audio = []\n",
    "\n",
    "stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=sample_rate,\n",
    "                input=True,\n",
    "                input_device_index=selected_device_id,\n",
    "                frames_per_buffer=1024)\n",
    "\n",
    "for _ in range(0, int(sample_rate / 1024 * duration)):\n",
    "    data = stream.read(1024)\n",
    "    audio.append(data)\n",
    "\n",
    "print(\"Recording complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the recorded audio as a WAV file\n",
    "# Define the output directory and filename for saving the recorded audio\n",
    "output_dir = \"recordings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"recorded_audio.wav\")\n",
    "\n",
    "with wave.open(output_file, 'wb') as wf:\n",
    "    wf.setnchannels(1)  # Mono audio\n",
    "    wf.setsampwidth(2)  # 2 bytes per sample\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b''.join(audio))\n",
    "\n",
    "print(f\"Audio saved as {output_file}\")\n",
    "\n",
    "# Close the audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the recorded audio using whisper\n",
    "# signal, _ = whisper.load(output_file)\n",
    "\n",
    "# Load the recorded audio file\n",
    "audio_file = \"recordings/recorded_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the audio file\n",
    "with wave.open(audio_file, 'rb') as wf:\n",
    "    # Get audio data\n",
    "    audio_data = wf.readframes(-1)\n",
    "    audio_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "    # Get sample rate and duration\n",
    "    sample_rate = wf.getframerate()\n",
    "    duration = len(audio_data) / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time array for x-axis (in seconds)\n",
    "time = np.linspace(0, duration, len(audio_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the audio waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(time, audio_data, lw=0.5)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Recorded Audio Waveform\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust NumPy print options to display the full array without truncation\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the numerical representation of the audio\n",
    "print(\"Numerical representation of the recorded audio:\")\n",
    "print(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now perform voice recognition using the extracted features\n",
    "# This may involve training a machine learning model or using a pre-trained model\n",
    "# The specific recognition process will depend on your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
