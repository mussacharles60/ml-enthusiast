{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model file for Tensorflow Lite for mobile_face_net\n",
    "model_file = 'models/mobile_face_net.tflite'\n",
    "\n",
    "# An input file to load the stored embeddings\n",
    "input_file = 'embeddings/embeddings.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the JSON file\n",
    "with open(input_file, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data)\n",
    "\n",
    "user_name = data['name']\n",
    "stored_embedding = np.array(data['embedding'])\n",
    "\n",
    "# Load the TensorFlow Lite model for mobile_face_net\n",
    "interpreter = tf.lite.Interpreter(model_path=model_file)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera feed\n",
    "cap = cv2.VideoCapture(0)  # 0 for the default camera, change as needed\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Detect faces in the camera frame\n",
    "    # You can use a face detection model here\n",
    "    \n",
    "    # Assuming 'faces' is a list of detected faces (each is a bounding box)\n",
    "    for face_bbox in faces:\n",
    "        # Extract the face region\n",
    "        x, y, w, h = face_bbox\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Preprocess the face image for the mobile_face_net model\n",
    "        face_image = cv2.resize(face_image, (112, 112))\n",
    "        face_image = (face_image - 127.5) / 128.0  # Normalize pixel values to [-1, 1]\n",
    "        face_image = np.expand_dims(face_image, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Perform inference with the mobile_face_net model\n",
    "        interpreter.set_tensor(input_details[0]['index'], face_image)\n",
    "        interpreter.invoke()\n",
    "        embeddings = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Calculate similarity between the detected face and stored embedding\n",
    "        similarity = np.dot(embeddings, stored_embedding.T)\n",
    "        \n",
    "        # Set a threshold for face recognition\n",
    "        threshold = 0.7  # Adjust as needed\n",
    "        if similarity > threshold:\n",
    "            # Look up the individual's name based on individual_id\n",
    "            \n",
    "            # Draw a label on the detected face\n",
    "            cv2.putText(frame, user_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            print(f\"User {user_name} face detected!\")\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
