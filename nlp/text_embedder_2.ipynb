{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning: \n",
    "# from keras.models import Input, Model\n",
    "# from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# Custom functions\n",
    "from utility import text_preprocessing, create_unique_word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "texts = [x for x in texts['text']]\n",
    "\n",
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "\n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "\n",
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s\n",
    "# X = sparse.csr_matrix(X)\n",
    "# Y = sparse.csr_matrix(Y)\n",
    "\n",
    "# # Defining the size of the embedding\n",
    "# embed_size = 2\n",
    "\n",
    "# # Defining the neural network\n",
    "# inp = Input(shape=(X.shape[1],))\n",
    "# x = Dense(units=embed_size, activation='linear')(inp)\n",
    "# x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "# model = Model(inputs=inp, outputs=x)\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# ----------------------\n",
    "X = tf.cast(X, dtype=tf.float32)\n",
    "Y = tf.cast(Y, dtype=tf.float32)\n",
    "\n",
    "# Defining the size of the embedding\n",
    "embed_size = 2\n",
    "\n",
    "# Input shape should match the dimensionality of your one-hot encoded vectors\n",
    "inp = Input(shape=(n_words,))  # Assuming n_words is the dimensionality of your one-hot encoded vectors\n",
    "\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=n_words, activation='softmax')(x)  # Output has n_words units to match one-hot encoding\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# ----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=5000,\n",
    "    epochs=500\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })\n",
    "\n",
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')        \n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder: {e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt', 'w') as f:\n",
    "    for key, value in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')   \n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
